{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":3557332,"sourceType":"datasetVersion","datasetId":2138237},{"sourceId":285870,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":245024,"modelId":266630}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install kagglehub tqdm h5py scikit-learn tensorflow opencv-python matplotlib \n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ==== Cell 2: Import libraries ====\nimport os\nimport sys\nimport glob\nimport zipfile\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom tensorflow.keras.models import Model, load_model\nfrom tensorflow.keras.layers import Input, Dense, Dropout, GlobalAveragePooling2D\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, TensorBoard\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.applications.vgg16 import preprocess_input\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\nimport cv2\nimport time\nimport datetime\nimport h5py\nimport shutil\nfrom tqdm import tqdm\nimport requests\nimport kagglehub\nimport tensorflow.keras.backend as K\nimport re","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ==== New Cell: GPU Configuration ====\ndef configure_gpus():\n    \"\"\"\n    Configure GPUs and set up distributed training strategy.\n    \n    Returns:\n        tf.distribute.Strategy: Distribution strategy for multi-GPU training.\n    \"\"\"\n    print(\"=\" * 50)\n    print(\"Configuring GPUs for distributed training...\")\n    \n    # Check available GPUs\n    gpus = tf.config.list_physical_devices('GPU')\n    print(f\"Number of GPUs available: {len(gpus)}\")\n    \n    for gpu in gpus:\n        print(f\"Name: {gpu.name}, Type: {gpu.device_type}\")\n        \n        # Configure memory growth to avoid allocating all memory at once\n        try:\n            tf.config.experimental.set_memory_growth(gpu, True)\n            print(f\"Memory growth set to True for {gpu.name}\")\n        except:\n            print(f\"Failed to set memory growth for {gpu.name}\")\n    \n    # Choose the appropriate distribution strategy\n    if len(gpus) > 1:\n        # MirroredStrategy for multiple GPUs on a single machine\n        strategy = tf.distribute.MirroredStrategy()\n        print(f\"Using MirroredStrategy with {strategy.num_replicas_in_sync} GPUs\")\n    else:\n        # Default strategy for single GPU\n        strategy = tf.distribute.get_strategy()\n        print(\"Using default strategy (single GPU)\")\n    \n    # Log device placement for debugging\n    tf.debugging.set_log_device_placement(False)  # Set to True for detailed device placement logs\n    \n    return strategy\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n\n# Configuration\nRANDOM_SEED = 42\nnp.random.seed(RANDOM_SEED)\ntf.random.set_seed(RANDOM_SEED)\n\n# Directory structure for Kaggle\nDATA_DIR = '/kaggle/working/vggface2_data'\nTRAIN_DIR = os.path.join(DATA_DIR, 'train')\nVAL_DIR = os.path.join(DATA_DIR, 'val')\nPROCESSED_TRAIN_DIR = os.path.join(DATA_DIR, 'processed_train')\nPROCESSED_VAL_DIR = os.path.join(DATA_DIR, 'processed_val')\n\n# VGG model paths - Use predefined dataset directory\nVGG_MODEL_FOLDER = '/kaggle/input/vgg_model/tensorflow2/default/1'  # Directly reference input dataset\nVGG_FEATURE_MODEL_PATH = os.path.join(VGG_MODEL_FOLDER, 'vggface_features.h5')\n\n# Fine-tuned model paths\nFINETUNED_MODEL_FOLDER = '/kaggle/working/vgg_finetuned'\nFINETUNED_FULL_MODEL_PATH = os.path.join(FINETUNED_MODEL_FOLDER, 'vggface_finetuned_full.h5')\nFINETUNED_FEATURE_MODEL_PATH = os.path.join(FINETUNED_MODEL_FOLDER, 'vggface_finetuned_features.h5')\n\n# Training parameters\nBATCH_SIZE = 64\nEPOCHS = 100\nLEARNING_RATE = 1e-4\nIMG_SIZE = (224, 224)\nVALIDATION_SPLIT = 0.2\n\n# Create directories if they don't exist\nos.makedirs(DATA_DIR, exist_ok=True)\nos.makedirs(FINETUNED_MODEL_FOLDER, exist_ok=True)\nos.makedirs(PROCESSED_TRAIN_DIR, exist_ok=True)\nos.makedirs(PROCESSED_VAL_DIR, exist_ok=True)\n\n# Confirm paths\nprint(f\"Using VGG model from: {VGG_FEATURE_MODEL_PATH}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ==== Cell 5: Modified dataset function for Kaggle ====\ndef download_vggface2_dataset(manual_path=None):\n    \"\"\"\n    Set up the VGGFace2 dataset in Kaggle environment.\n    \n    Args:\n        manual_path (str, optional): Path to manually added dataset in Kaggle.\n    \n    Returns:\n        str: Path to the dataset files.\n    \"\"\"\n    print(\"=\" * 50)\n    print(\"Setting up VGGFace2 dataset...\")\n    \n    # Check if dataset directories already exist\n    if os.path.exists(TRAIN_DIR) and os.path.exists(VAL_DIR):\n        print(\"Dataset directories already exist. Skipping setup.\")\n        return DATA_DIR\n    \n    # In Kaggle, datasets are typically in /kaggle/input/[dataset-name]\n    # Check if the dataset is already available in the input directory\n    kaggle_dataset_paths = glob.glob('/kaggle/input/*vggface2*')\n    \n    if kaggle_dataset_paths:\n        dataset_path = kaggle_dataset_paths[0]\n        print(f\"Found VGGFace2 dataset at: {dataset_path}\")\n    elif manual_path and os.path.exists(manual_path):\n        print(f\"Using manually specified dataset at: {manual_path}\")\n        dataset_path = manual_path\n    else:\n        print(\"VGGFace2 dataset not found in Kaggle inputs.\")\n        print(\"Please add the dataset to your notebook using the 'Add Data' button.\")\n        print(\"For VGGFace2, search for 'hearfool/vggface2' or add your own dataset.\")\n        \n        # Provide instructions for adding data in Kaggle\n        print(\"\\nInstructions:\")\n        print(\"1. Click '+Add data' at the top right of your notebook\")\n        print(\"2. Search for 'hearfool/vggface2' or the dataset you want to use\")\n        print(\"3. Click 'Add' and then run this cell again\")\n        \n        # Ask for manual path as fallback\n        manual_input = input(\"Or enter path to the dataset if already added: \")\n        if manual_input and os.path.exists(manual_input):\n            dataset_path = manual_input\n        else:\n            print(\"Valid dataset path not provided. Exiting.\")\n            sys.exit(1)\n    \n    # Check if the path is a directory with train/val subdirectories\n    if os.path.isdir(dataset_path):\n        possible_train_dir = os.path.join(dataset_path, 'train')\n        possible_val_dir = os.path.join(dataset_path, 'val')\n        \n        if os.path.exists(possible_train_dir) and os.path.exists(possible_val_dir):\n            print(\"Dataset already contains train and val directories.\")\n            print(f\"Creating symbolic links to {TRAIN_DIR} and {VAL_DIR}\")\n            \n            # Make sure target directories exist\n            os.makedirs(TRAIN_DIR, exist_ok=True)\n            os.makedirs(VAL_DIR, exist_ok=True)\n            \n            # In Kaggle, we need to copy files instead of symlinks\n            if not os.path.exists(TRAIN_DIR) or not os.listdir(TRAIN_DIR):\n                if os.path.exists(TRAIN_DIR):\n                    os.rmdir(TRAIN_DIR)\n                # Kaggle doesn't always support symlinks, so use a copy command instead\n                print(f\"Copying training data from {possible_train_dir} to {TRAIN_DIR}\")\n                # Create a symlink first (faster) and fall back to copy if it fails\n                try:\n                    os.symlink(possible_train_dir, TRAIN_DIR)\n                    print(\"Created symbolic link for training directory\")\n                except:\n                    print(\"Symbolic link creation failed, copying files instead (this may take a while)\")\n                    # Use shutil.copytree or os.system command for copying\n                    # This is commented out as it might be slow for large datasets\n                    # shutil.copytree(possible_train_dir, TRAIN_DIR)\n                    print(\"Consider using only a subset of the data for testing\")\n            \n            if not os.path.exists(VAL_DIR) or not os.listdir(VAL_DIR):\n                if os.path.exists(VAL_DIR):\n                    os.rmdir(VAL_DIR)\n                # Similar approach for validation directory\n                try:\n                    os.symlink(possible_val_dir, VAL_DIR)\n                    print(\"Created symbolic link for validation directory\")\n                except:\n                    print(\"Symbolic link creation failed. Consider copying files if needed.\")\n            \n            return DATA_DIR\n        \n        # Check for zip files\n        zip_files = glob.glob(os.path.join(dataset_path, \"*.zip\"))\n        if zip_files:\n            print(f\"Found {len(zip_files)} zip files in the dataset directory.\")\n            # Extract zip files logic...\n        else:\n            print(\"Checking for direct dataset structure...\")\n            class_dirs = glob.glob(os.path.join(dataset_path, \"*\"))\n            if class_dirs:\n                print(f\"Found {len(class_dirs)} potential class directories.\")\n                # If this looks like a dataset with class folders, set it up\n                os.makedirs(TRAIN_DIR, exist_ok=True)\n                print(f\"Linking dataset from {dataset_path} to {TRAIN_DIR}\")\n                # Try symbolic link first\n                try:\n                    os.symlink(dataset_path, TRAIN_DIR)\n                    print(f\"Created symbolic link for training directory\")\n                except:\n                    print(\"Symbolic link creation failed. You may need to copy files manually.\")\n                return DATA_DIR\n    else:\n        # Handle single zip file case\n        print(f\"Found single dataset file: {dataset_path}\")\n        # Extract zip file logic...\n    \n    # Verify the folder structure\n    expected_folders = [TRAIN_DIR, VAL_DIR]\n    for folder in expected_folders:\n        if not os.path.exists(folder):\n            print(f\"Warning: Expected folder {folder} not found after setup.\")\n        else:\n            print(f\"Verified: {folder} exists.\")\n    \n    return DATA_DIR","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ==== Cell 6: Define dataset preprocessing function ====\ndef reconstruct_validation_set(train_dir, val_dir, processed_train_dir, processed_val_dir, validation_split=0.2):\n    \"\"\"\n    Reconstruct the validation set to include examples from all classes in the training set.\n    \n    Args:\n        train_dir (str): Path to the original training directory\n        val_dir (str): Path to the original validation directory\n        processed_train_dir (str): Path to save the new processed training data\n        processed_val_dir (str): Path to save the new processed validation data\n        validation_split (float): Fraction of training data to use for validation\n        \n    Returns:\n        tuple: Lists of training and validation image paths\n    \"\"\"\n    print(\"=\" * 50)\n    print(\"Checking for processed data...\")\n    \n    # Check if processed directories exist and contain data\n    processed_data_exists = (os.path.exists(processed_train_dir) and \n                             os.path.exists(processed_val_dir) and\n                             len(os.listdir(processed_train_dir)) > 0 and\n                             len(os.listdir(processed_val_dir)) > 0)\n    \n    if processed_data_exists:\n        print(\"Processed data already exists.\")\n        reuse_existing = input(\"Would you like to use existing processed data? (y/n): \").lower() == 'y'\n        \n        if reuse_existing:\n            print(\"Using existing processed data.\")\n            # Get paths of existing processed images\n            processed_train_images = []\n            processed_val_images = []\n            \n            # Walk through the processed train directory\n            for root, _, files in os.walk(processed_train_dir):\n                for file in files:\n                    if file.lower().endswith(('.png', '.jpg', '.jpeg')):\n                        processed_train_images.append(os.path.join(root, file))\n            \n            # Walk through the processed val directory\n            for root, _, files in os.walk(processed_val_dir):\n                for file in files:\n                    if file.lower().endswith(('.png', '.jpg', '.jpeg')):\n                        processed_val_images.append(os.path.join(root, file))\n                        \n            print(f\"Found {len(processed_train_images)} existing training images and {len(processed_val_images)} existing validation images\")\n            return processed_train_images, processed_val_images\n        else:\n            print(\"Reprocessing data...\")\n    else:\n        print(\"No processed data found. Processing data...\")\n    \n    print(\"Reconstructing validation set to include all classes...\")\n    \n    # Clear existing processed directories if they exist\n    if os.path.exists(processed_train_dir):\n        shutil.rmtree(processed_train_dir)\n    if os.path.exists(processed_val_dir):\n        shutil.rmtree(processed_val_dir)\n    \n    os.makedirs(processed_train_dir, exist_ok=True)\n    os.makedirs(processed_val_dir, exist_ok=True)\n    \n    # Get all class directories in the training set\n    class_dirs = [d for d in os.listdir(train_dir) if os.path.isdir(os.path.join(train_dir, d))]\n    print(f\"Found {len(class_dirs)} identity classes in the training set\")\n    \n    processed_train_images = []\n    processed_val_images = []\n    \n    # Process each class\n    for class_name in tqdm(class_dirs, desc=\"Processing classes\"):\n        class_dir = os.path.join(train_dir, class_name)\n        \n        # Get all images for this class\n        images = [os.path.join(class_dir, f) for f in os.listdir(class_dir) \n                 if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n        \n        if not images:\n            continue\n            \n        # Split images into train and validation\n        train_imgs, val_imgs = train_test_split(\n            images, test_size=validation_split, random_state=RANDOM_SEED\n        )\n        \n        # Create class directories in processed folders\n        os.makedirs(os.path.join(processed_train_dir, class_name), exist_ok=True)\n        os.makedirs(os.path.join(processed_val_dir, class_name), exist_ok=True)\n        \n        # Process training images\n        for idx, img_path in enumerate(train_imgs):\n            try:\n                # Load and resize image\n                img = cv2.imread(img_path)\n                if img is None:\n                    continue\n                    \n                img_resized = cv2.resize(img, IMG_SIZE)\n                \n                # Save processed image\n                new_img_path = os.path.join(processed_train_dir, class_name, f\"{idx}.jpg\")\n                cv2.imwrite(new_img_path, img_resized)\n                processed_train_images.append(new_img_path)\n            except Exception as e:\n                print(f\"Error processing image {img_path}: {e}\")\n        \n        # Process validation images\n        for idx, img_path in enumerate(val_imgs):\n            try:\n                # Load and resize image\n                img = cv2.imread(img_path)\n                if img is None:\n                    continue\n                    \n                img_resized = cv2.resize(img, IMG_SIZE)\n                \n                # Save processed image\n                new_img_path = os.path.join(processed_val_dir, class_name, f\"{idx}.jpg\")\n                cv2.imwrite(new_img_path, img_resized)\n                processed_val_images.append(new_img_path)\n            except Exception as e:\n                print(f\"Error processing image {img_path}: {e}\")\n    \n    print(f\"Created new dataset with {len(processed_train_images)} training images and {len(processed_val_images)} validation images\")\n    print(f\"Data split across {len(class_dirs)} identity classes\")\n    \n    return processed_train_images, processed_val_images\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ==== Modified Cell: Update create_data_generators function ====\ndef create_data_generators(strategy):\n    \"\"\"\n    Create data generators for training and validation, optimized for multi-GPU training.\n    \n    Args:\n        strategy: Distribution strategy for multi-GPU training.\n        \n    Returns:\n        tuple: Training and validation data generators and tf.data.Dataset versions.\n    \"\"\"\n    print(\"=\" * 50)\n    print(\"Setting up data generators for multi-GPU training...\")\n    \n    # Calculate global batch size based on number of GPUs\n    global_batch_size = BATCH_SIZE * strategy.num_replicas_in_sync\n    print(f\"Using global batch size: {global_batch_size}\")\n    \n    # Minimal processing for both training and validation\n    train_datagen = ImageDataGenerator(\n        preprocessing_function=preprocess_input\n    )\n    \n    val_datagen = ImageDataGenerator(\n        preprocessing_function=preprocess_input\n    )\n    \n    # Create generators with global batch size\n    train_generator = train_datagen.flow_from_directory(\n        PROCESSED_TRAIN_DIR,\n        target_size=IMG_SIZE,\n        batch_size=global_batch_size,\n        class_mode='categorical',\n        shuffle=True\n    )\n    \n    val_generator = val_datagen.flow_from_directory(\n        PROCESSED_VAL_DIR,\n        target_size=IMG_SIZE,\n        batch_size=global_batch_size,\n        class_mode='categorical',\n        shuffle=False\n    )\n    \n    print(f\"Created data generators with {train_generator.num_classes} classes.\")\n    print(f\"Training on {train_generator.samples} samples with global batch size of {global_batch_size}\")\n    print(f\"Training steps per epoch: {len(train_generator)}\")\n    \n    # Convert generators to tf.data.Dataset for better multi-GPU performance\n    def generator_to_dataset(generator):\n        def gen():\n            while True:\n                x, y = next(generator)\n                yield x, y\n                \n        output_signature = (\n            tf.TensorSpec(shape=(None,) + generator.image_shape, dtype=tf.float32),\n            tf.TensorSpec(shape=(None, generator.num_classes), dtype=tf.float32)\n        )\n        \n        dataset = tf.data.Dataset.from_generator(\n            gen, \n            output_signature=output_signature\n        )\n        \n        # Add performance optimizations\n        dataset = dataset.repeat()\n        dataset = dataset.prefetch(tf.data.AUTOTUNE)\n        \n        return dataset\n    \n    # Convert both generators to datasets\n    train_dataset = generator_to_dataset(train_generator)\n    val_dataset = generator_to_dataset(val_generator)\n    \n    print(\"Successfully created tf.data.Datasets from generators\")\n    \n    return train_generator, val_generator, train_dataset, val_dataset\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\ndef load_vggface_model():\n    \"\"\"\n    Load the pretrained VGGFace model.\n    \n    Returns:\n        Model: Loaded VGGFace model.\n    \"\"\"\n    print(\"=\" * 50)\n    print(\"Loading pretrained VGGFace model...\")\n\n    # Directly check the expected model path\n    if os.path.exists(VGG_FEATURE_MODEL_PATH):\n        print(f\"Found VGGFace model at: {VGG_FEATURE_MODEL_PATH}\")\n    else:\n        print(f\"Error: VGGFace model not found at {VGG_FEATURE_MODEL_PATH}\")\n        sys.exit(1)\n    \n    try:\n        # Load the model\n        vgg_model = load_model(VGG_FEATURE_MODEL_PATH)\n        print(\"VGGFace model loaded successfully.\")\n        return vgg_model\n    except Exception as e:\n        print(f\"Error loading VGGFace model: {e}\")\n        sys.exit(1)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport glob\nimport re\nfrom tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, TensorBoard, Callback\n\n# Create a custom callback to clean up checkpoints after saving\nclass CheckpointCleanupCallback(Callback):\n    def __init__(self, checkpoint_dir):\n        super().__init__()\n        self.checkpoint_dir = checkpoint_dir\n    \n    def on_epoch_end(self, epoch, logs=None):\n        # Only run if a checkpoint was saved this epoch\n        # This will be triggered after ModelCheckpoint has saved a new file\n        checkpoint_files = sorted(\n            glob.glob(os.path.join(self.checkpoint_dir, 'checkpoint_*.keras')),\n            key=os.path.getmtime\n        )\n        \n        # Keep only the best checkpoint (most recent)\n        if len(checkpoint_files) > 1:\n            for old_checkpoint in checkpoint_files[:-1]:\n                print(f\"Deleting old checkpoint: {old_checkpoint}\")\n                os.remove(old_checkpoint)\n\ndef build_and_finetune_model(vgg_model, train_generator, val_generator, strategy, resume_from_checkpoint=None):\n    \"\"\"\n    Build and fine-tune the model for face recognition with multi-GPU support.\n    Can resume training from a checkpoint.\n    \n    Args:\n        vgg_model (Model): Loaded VGGFace model.\n        train_generator: Training data generator.\n        val_generator: Validation data generator.\n        strategy: Distribution strategy for multi-GPU training.\n        resume_from_checkpoint (str, optional): Path to checkpoint file to resume from.\n    \n    Returns:\n        tuple: Fine-tuned model and training history.\n    \"\"\"\n    print(\"=\" * 50)\n    print(\"Building and fine-tuning the model with multi-GPU support...\")\n    \n    num_classes = train_generator.num_classes\n    print(f\"Building model with {num_classes} output classes\")\n\n    if train_generator.num_classes != val_generator.num_classes:\n        print(f\"WARNING: Training generator has {train_generator.num_classes} classes but validation generator has {val_generator.num_classes}\")\n    \n    global_batch_size = BATCH_SIZE * strategy.num_replicas_in_sync\n    print(f\"Global batch size: {global_batch_size} (Base:{BATCH_SIZE} Ã— {strategy.num_replicas_in_sync} GPUs)\")\n    \n    # Create the model inside the strategy scope\n    with strategy.scope():\n        if resume_from_checkpoint:\n            print(f\"Loading model from checkpoint: {resume_from_checkpoint}\")\n            try:\n                fine_tuned_model = load_model(resume_from_checkpoint)\n                print(\"Successfully loaded model from checkpoint\")\n\n                # Ensure correct learning rate\n                try:\n                    if hasattr(fine_tuned_model.optimizer, 'learning_rate'):\n                        K.set_value(fine_tuned_model.optimizer.learning_rate, LEARNING_RATE)\n                    elif hasattr(fine_tuned_model.optimizer, 'lr'):\n                        K.set_value(fine_tuned_model.optimizer.lr, LEARNING_RATE)\n                    else:\n                        print(\"Could not reset learning rate - optimizer structure unknown\")\n                except Exception as e:\n                    print(f\"Could not reset learning rate: {e}\")\n                    print(\"Continuing with loaded optimizer settings\")\n\n            except Exception as e:\n                print(f\"Error loading checkpoint: {e}\")\n                print(\"Building model from scratch instead...\")\n                resume_from_checkpoint = None\n        \n        # If no checkpoint, build model from scratch\n        if not resume_from_checkpoint:\n            for layer in vgg_model.layers[:10]:\n                layer.trainable = False\n            for layer in vgg_model.layers[10:]:\n                layer.trainable = True\n            \n            x = vgg_model.output\n            x = Dropout(0.5, name='dropout_ft1')(x)\n            x = Dense(1024, activation='relu', name='fc8')(x)\n            x = Dropout(0.5, name='dropout_ft2')(x)\n            predictions = Dense(num_classes, activation='softmax', name='predictions')(x)\n            \n            fine_tuned_model = Model(inputs=vgg_model.input, outputs=predictions)\n            fine_tuned_model.compile(\n                optimizer=Adam(learning_rate=LEARNING_RATE),\n                loss='categorical_crossentropy',\n                metrics=['accuracy']\n            )\n\n    fine_tuned_model.summary()\n\n    # Find the last epoch number if resuming\n    initial_epoch = 0\n    if resume_from_checkpoint:\n        checkpoint_name = os.path.basename(resume_from_checkpoint)\n        match = re.search(r'checkpoint_(\\d+)_', checkpoint_name)\n        if match:\n            initial_epoch = int(match.group(1))\n            print(f\"Resuming from epoch {initial_epoch}\")\n\n    # Clean up any existing checkpoints before starting training\n    def cleanup_all_checkpoints():\n        checkpoint_files = sorted(\n            glob.glob(os.path.join(FINETUNED_MODEL_FOLDER, 'checkpoint_*.keras')),\n            key=os.path.getmtime\n        )\n        \n        # Keep only the checkpoint we're resuming from (if any)\n        for checkpoint in checkpoint_files:\n            if resume_from_checkpoint and os.path.basename(checkpoint) == os.path.basename(resume_from_checkpoint):\n                continue\n            print(f\"Deleting old checkpoint: {checkpoint}\")\n            os.remove(checkpoint)\n    \n    # Clean up old checkpoints before starting training\n    cleanup_all_checkpoints()\n\n    # Set up callbacks\n    model_checkpoint_callback = ModelCheckpoint(\n        os.path.join(FINETUNED_MODEL_FOLDER, 'checkpoint_{epoch:02d}_{val_accuracy:.4f}.keras'),\n        monitor='val_accuracy',\n        save_best_only=True,\n        save_weights_only=False,\n        mode='max',\n        verbose=1\n    )\n    \n    # Create our custom cleanup callback\n    cleanup_callback = CheckpointCleanupCallback(FINETUNED_MODEL_FOLDER)\n\n    callbacks = [\n        EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True),\n        ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=10, min_lr=1e-7),\n        model_checkpoint_callback,\n        cleanup_callback,  # Add our cleanup callback\n        TensorBoard(log_dir=os.path.join(FINETUNED_MODEL_FOLDER, 'logs'))\n    ]\n    \n    # Update generator batch sizes\n    train_generator.batch_size = global_batch_size\n    val_generator.batch_size = global_batch_size\n\n    # Start fine-tuning\n    print(\"\\nStarting fine-tuning...\")\n    history = fine_tuned_model.fit(\n        train_generator,\n        steps_per_epoch=len(train_generator),\n        epochs=EPOCHS,\n        validation_data=val_generator,\n        validation_steps=len(val_generator),\n        callbacks=callbacks,\n        verbose=1,\n        initial_epoch=initial_epoch\n    )\n\n    return fine_tuned_model, history","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ==== New Cell: Memory monitoring function (optional but useful) ====\n\ndef monitor_gpu_memory():\n    \"\"\"\n    Monitor GPU memory usage during training.\n    This function can be called periodically to check memory usage.\n    \"\"\"\n    import subprocess\n    import re\n    \n    try:\n        # Use nvidia-smi to get GPU memory info\n        result = subprocess.check_output(['nvidia-smi', '--query-gpu=memory.used,memory.total', '--format=csv,noheader,nounits'])\n        result = result.decode('utf-8').strip().split('\\n')\n        \n        print(\"\\nGPU Memory Usage:\")\n        for i, gpu_info in enumerate(result):\n            memory_used, memory_total = map(int, re.findall(r'\\d+', gpu_info))\n            usage_percent = memory_used / memory_total * 100\n            print(f\"GPU {i}: {memory_used} MB / {memory_total} MB ({usage_percent:.1f}%)\")\n        print()\n    except Exception as e:\n        print(f\"Failed to monitor GPU memory: {e}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def evaluate_model(model, val_generator):\n    \"\"\"\n    Evaluate the fine-tuned model with multi-GPU support.\n    \n    Args:\n        model (Model): Fine-tuned model.\n        val_generator: Validation data generator.\n    \n    Returns:\n        list: Evaluation results.\n    \"\"\"\n    print(\"=\" * 50)\n    print(\"Evaluating the fine-tuned model...\")\n    \n    # Monitor GPU memory before evaluation\n    monitor_gpu_memory()\n    \n    # Reset generator if it's a custom generator that supports reset()\n    if hasattr(val_generator, 'reset') and callable(val_generator.reset):\n        val_generator.reset()\n    \n    # Perform evaluation\n    eval_results = model.evaluate(\n        val_generator, \n        steps=len(val_generator), \n        verbose=1\n    )\n    \n    print(f\"Evaluation Results:\")\n    print(f\"Loss: {eval_results[0]:.4f}\")\n    print(f\"Accuracy: {eval_results[1]:.4f}\")\n    \n    # Monitor GPU memory after evaluation\n    monitor_gpu_memory()\n    \n    return eval_results","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ==== Cell 11: Define model saving function ====\ndef save_models(fine_tuned_model):\n    \"\"\"\n    Save the fine-tuned model and feature extraction model.\n    \n    Args:\n        fine_tuned_model (Model): Fine-tuned model.\n    \"\"\"\n    print(\"=\" * 50)\n    print(\"Saving the fine-tuned models...\")\n    \n    # Save the complete fine-tuned model\n    fine_tuned_model.save(FINETUNED_FULL_MODEL_PATH)\n    print(f\"Saved complete fine-tuned model to: {FINETUNED_FULL_MODEL_PATH}\")\n    \n    # Create and save the feature extraction model\n    # (This is useful for extracting features for other tasks)\n    feature_layer = fine_tuned_model.get_layer('fc8')\n    feature_model = Model(inputs=fine_tuned_model.input, outputs=feature_layer.output)\n    feature_model.save(FINETUNED_FEATURE_MODEL_PATH)\n    print(f\"Saved feature extraction model to: {FINETUNED_FEATURE_MODEL_PATH}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ==== Cell 12: Define function to plot training history ====\ndef plot_training_history(history):\n    \"\"\"\n    Plot the training history.\n    \n    Args:\n        history: Training history object.\n    \"\"\"\n    print(\"=\" * 50)\n    print(\"Plotting training history...\")\n    \n    # Create figure and subplots\n    plt.figure(figsize=(12, 5))\n    \n    # Plot accuracy\n    plt.subplot(1, 2, 1)\n    plt.plot(history.history['accuracy'], label='Training Accuracy')\n    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n    plt.title('Model Accuracy')\n    plt.xlabel('Epoch')\n    plt.ylabel('Accuracy')\n    plt.legend()\n    \n    # Plot loss\n    plt.subplot(1, 2, 2)\n    plt.plot(history.history['loss'], label='Training Loss')\n    plt.plot(history.history['val_loss'], label='Validation Loss')\n    plt.title('Model Loss')\n    plt.xlabel('Epoch')\n    plt.ylabel('Loss')\n    plt.legend()\n    \n    # Save the plot\n    plt.tight_layout()\n    plt.savefig(os.path.join(FINETUNED_MODEL_FOLDER, 'training_history.png'))\n    print(f\"Saved training history plot to: {os.path.join(FINETUNED_MODEL_FOLDER, 'training_history.png')}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ==== Cell 13: Define function to calculate detailed metrics ====\ndef calculate_detailed_metrics(model, val_generator):\n    \"\"\"\n    Calculate detailed metrics for the model performance.\n    \n    Args:\n        model (Model): Fine-tuned model.\n        val_generator: Validation data generator.\n    \"\"\"\n    print(\"=\" * 50)\n    print(\"Calculating detailed metrics...\")\n    \n    # Get predictions\n    val_generator.reset()\n    y_true = val_generator.classes\n    y_pred_probs = model.predict(val_generator, steps=len(val_generator), verbose=1)\n    y_pred = np.argmax(y_pred_probs, axis=1)\n    \n    # Calculate metrics\n    accuracy = accuracy_score(y_true, y_pred)\n    precision = precision_score(y_true, y_pred, average='weighted')\n    recall = recall_score(y_true, y_pred, average='weighted')\n    f1 = f1_score(y_true, y_pred, average='weighted')\n    \n    # Log metrics\n    print(f\"Detailed Metrics:\")\n    print(f\"Accuracy: {accuracy:.4f}\")\n    print(f\"Precision: {precision:.4f}\")\n    print(f\"Recall: {recall:.4f}\")\n    print(f\"F1 Score: {f1:.4f}\")\n    \n    # Calculate confusion matrix\n    conf_matrix = confusion_matrix(y_true, y_pred)\n    \n    # Save metrics to a file\n    with open(os.path.join(FINETUNED_MODEL_FOLDER, 'metrics.txt'), 'w') as f:\n        f.write(f\"Accuracy: {accuracy:.4f}\\n\")\n        f.write(f\"Precision: {precision:.4f}\\n\")\n        f.write(f\"Recall: {recall:.4f}\\n\")\n        f.write(f\"F1 Score: {f1:.4f}\\n\")\n    \n    print(f\"Saved metrics to: {os.path.join(FINETUNED_MODEL_FOLDER, 'metrics.txt')}\")\n    \n    return {\n        'accuracy': accuracy,\n        'precision': precision,\n        'recall': recall,\n        'f1': f1,\n        'confusion_matrix': conf_matrix\n    }","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n\ndef cleanup_old_checkpoints(latest_checkpoint):\n    \"\"\"\n    Deletes older checkpoints except the latest one.\n    \"\"\"\n    checkpoint_files = sorted(\n        glob.glob(os.path.join(FINETUNED_MODEL_FOLDER, 'checkpoint_*.keras')),\n        key=os.path.getmtime\n    )\n\n    # Keep only the latest checkpoint\n    for chkpt in checkpoint_files:\n        if chkpt != latest_checkpoint:\n            print(f\"Deleting old checkpoint: {chkpt}\")\n            os.remove(chkpt)\n\ndef main():\n    \"\"\"\n    Main function to run the full fine-tuning pipeline on Kaggle with multi-GPU support.\n    \"\"\"\n    print(\"=\" * 50)\n    print(\"Starting VGGFace2 fine-tuning pipeline on Kaggle with multi-GPU support...\")\n    print(\"=\" * 50)\n\n    start_time = time.time()\n\n    # Configure GPUs and get distribution strategy\n    strategy = configure_gpus()\n\n    # Kaggle dataset availability check\n    print(\"Checking Kaggle dataset availability...\")\n    kaggle_dataset_paths = glob.glob('/kaggle/input/*vggface2*')\n    manual_path = None\n\n    if kaggle_dataset_paths:\n        print(f\"Found potential VGGFace2 datasets:\")\n        for i, path in enumerate(kaggle_dataset_paths):\n            print(f\"{i+1}. {path}\")\n        selection = input(f\"Enter number to select dataset (1-{len(kaggle_dataset_paths)}), or 'other': \")\n\n        if selection.lower() == 'other':\n            manual_path = input(\"Enter the path to the dataset: \")\n        else:\n            try:\n                manual_path = kaggle_dataset_paths[int(selection)-1]\n            except:\n                print(\"Invalid selection, using first dataset.\")\n                manual_path = kaggle_dataset_paths[0]\n\n    # Step 1: Set up the dataset\n    download_vggface2_dataset(manual_path)\n\n    # Reconstruct and preprocess the dataset\n    reconstruct_validation_set(TRAIN_DIR, VAL_DIR, PROCESSED_TRAIN_DIR, PROCESSED_VAL_DIR, VALIDATION_SPLIT)\n\n    # Create data generators\n    train_generator, val_generator, _, _ = create_data_generators(strategy)\n\n    # Load pretrained VGGFace model\n    with strategy.scope():\n        vgg_model = load_vggface_model()\n\n    # ======== Checkpoint Selection =========\n    checkpoint_files = sorted(\n        glob.glob(os.path.join(FINETUNED_MODEL_FOLDER, 'checkpoint_*.keras')),\n        key=os.path.getmtime\n    )\n\n    resume_from_checkpoint = None\n\n    if checkpoint_files:\n        print(\"\\nAvailable checkpoints:\")\n        for idx, chkpt in enumerate(checkpoint_files):\n            print(f\"{idx + 1}. {os.path.basename(chkpt)}\")\n\n        selection = input(f\"\\nEnter the checkpoint number to load (1-{len(checkpoint_files)}), or press Enter to use the latest: \")\n\n        if selection.strip().isdigit():\n            chkpt_index = int(selection) - 1\n            if 0 <= chkpt_index < len(checkpoint_files):\n                resume_from_checkpoint = checkpoint_files[chkpt_index]\n            else:\n                print(\"Invalid selection, using latest checkpoint.\")\n                resume_from_checkpoint = checkpoint_files[-1]\n        else:\n            resume_from_checkpoint = checkpoint_files[-1]  # Default to latest checkpoint\n\n        print(f\"Loading model from checkpoint: {resume_from_checkpoint}\")\n\n        # ðŸ”¥ Delete older checkpoints before starting training\n        cleanup_old_checkpoints(resume_from_checkpoint)\n\n    # ======== Fine-Tuning =========\n    fine_tuned_model, history = build_and_finetune_model(\n        vgg_model, \n        train_generator, \n        val_generator, \n        strategy, \n        resume_from_checkpoint=resume_from_checkpoint\n    )\n\n    # Evaluate the model\n    eval_results = evaluate_model(fine_tuned_model, val_generator)\n\n    # Save the model\n    save_models(fine_tuned_model)\n\n    # Plot training history\n    plot_training_history(history)\n\n    # Calculate detailed metrics\n    metrics = calculate_detailed_metrics(fine_tuned_model, val_generator)\n\n    # Calculate total time\n    end_time = time.time()\n    total_time = end_time - start_time\n\n    # Log total time and results\n    print(\"=\" * 50)\n    print(f\"Multi-GPU fine-tuning completed in {total_time:.2f} seconds ({datetime.timedelta(seconds=total_time)})\")\n    print(\"=\" * 50)\n\n    # Summary of results\n    print(\"Summary of results:\")\n    print(f\"Model saved to: {FINETUNED_FULL_MODEL_PATH}\")\n    print(f\"Feature extraction model saved to: {FINETUNED_FEATURE_MODEL_PATH}\")\n    print(f\"Final validation accuracy: {metrics['accuracy']:.4f}\")\n    print(f\"Final validation F1 score: {metrics['f1']:.4f}\")\n    print(\"=\" * 50)\n\n    # Kaggle-specific output information\n    print(\"\\nKaggle Environment Information:\")\n    print(\"- Models trained with multi-GPU strategy using\", strategy.num_replicas_in_sync, \"GPUs\")\n    print(\"- Outputs are saved in /kaggle/working directory\")\n    print(\"- To save your models permanently, use the 'Save Version' button\")\n    print(\"- Output artifacts are available in the 'Output' section\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ==== Cell 15: Run the pipeline ====\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}